{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ee7173e-c79d-48d7-ab2c-1185a66519d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0+cu113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\npytorch源码目录\\n/usr/local/lib/python3.8/dist-packages/torch\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "print(torch.__version__)\n",
    "\"\"\"\n",
    "pytorch源码目录\n",
    "/usr/local/lib/python3.8/dist-packages/torch\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f7cca3-b0e4-4c8b-b37f-2ec5c7753fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "## 超参数\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "num_heads = 8\n",
    "embed_len = 512\n",
    "batch_size = 8\n",
    "stack_len = 6\n",
    "drop_out = 0.1\n",
    "\n",
    "input_vocab_size = 7000\n",
    "output_vocab_size = 7000\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f5fa3a0-d654-49a5-a8ed-0ec9c2cc0c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embedding block\n",
    "class InputEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    输入embedding word_embedding+position_embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, input_vocab_size=7000, embed_len=512, dropout=0.1, device=device):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        # WordEmbeddingLayer\n",
    "        self.word_embedding_layer = nn.Embedding(self.input_vocab_size, self.embed_len)  \n",
    "        # PositionEmbeddingLayer\n",
    "        self.position_embedding_layer = nn.Embedding(self.input_vocab_size, self.embed_len) \n",
    "        # DropoutLayer\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # input.shape: [batch_size, seq_len], dtype: torch.int64\n",
    "        \n",
    "        # word_embedding\n",
    "        word_embedding = self.word_embedding_layer(input)\n",
    "        # position_embedding\n",
    "        batch_size, seq_len = input.shape\n",
    "        positions_vector = torch.arange(0, seq_len).expand(batch_size, seq_len).to(self.device)\n",
    "        positional_encoding = self.position_embedding_layer(positions_vector)\n",
    "        # output.shape: [bs, sl, embed_dim], dtype: torch.float32\n",
    "        return self.dropout_layer(word_embedding + positional_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "700a535c-c8b7-422e-b5d6-f44aa89324a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 20])\n",
      "torch.Size([8, 20, 512]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "seq_len = 20\n",
    "input = torch.randint(input_vocab_size, (batch_size, seq_len)).to(device)\n",
    "print(input.shape)\n",
    "embedding = InputEmbedding().to(device)\n",
    "output = embedding(input)\n",
    "print(output.shape, output.dtype)\n",
    "# print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8af1ca1-0e20-405c-a611-c3c9424ac1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaled dot-product Attention\n",
    "class ScaledDotProduct(nn.Module):\n",
    "    def __init__(self, embed_len=512, mask=None):\n",
    "        super(ScaledDotProduct, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.mask = mask\n",
    "        self.d_k = embed_len\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        queries.shape: [batch_size, num_heads, seq_len, head_length]\n",
    "        keys.shape: [batch_size, num_heads, seq_len, head_length]\n",
    "        values.shape: [batch_size, seq_len, num_heads, head_length]\n",
    "        \"\"\"\n",
    "        compatibility = torch.matmul(queries, torch.transpose(keys, 2, 3)) # [batch_size, num_heads, seq_len, seq_len]\n",
    "        compatibility = compatibility / math.sqrt(self.d_k)\n",
    "        \n",
    "        compatibility = self.softmax(compatibility)\n",
    "        if self.mask is not None:\n",
    "            compatibility = torch.tril(compatibility)\n",
    "        \n",
    "        return torch.matmul(compatibility, torch.transpose(values, 1, 2)) # [batch_size, num_heads, seq_len, head_length]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87441d2d-e0f2-4ecc-ba5c-fd065eebbbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## multihead attn block\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads=8, embed_len=512, mask=None):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.num_heads = num_heads\n",
    "        # self.batch_size = batch_size\n",
    "        self.mask = mask\n",
    "        self.head_length = int(self.embed_len/self.num_heads)\n",
    "        \n",
    "        self.q_in = self.k_in = self.v_in = self.embed_len\n",
    "        # 线性变换层\n",
    "        self.q_linear = nn.Linear(self.q_in, self.q_in)\n",
    "        self.k_linear = nn.Linear(self.k_in, self.k_in)\n",
    "        self.v_linear = nn.Linear(self.v_in, self.v_in)\n",
    "        \n",
    "        if self.mask is not None:\n",
    "            self.attn = ScaledDotProduct(mask=True)\n",
    "        else:\n",
    "            self.attn = ScaledDotProduct()\n",
    "        \n",
    "        self.output_linear = nn.Linear(self.q_in, self.q_in)\n",
    "    \n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        queries.shape: [batch_size, seq_len, embed_dim]\n",
    "        keys.shape: [batch_size, seq_len, embed_dim]\n",
    "        values.shape: [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        # 我们需要把QKV分拆为num_heads个头， [batch_size, seq_len, num_heads, head_length]\n",
    "        # 然后transpose(1, 2) -> [batch_size, num_heads, seq_len, head_length] 传入ScaledDotProduct\n",
    "        batch_size, seq_len = queries.shape[0], queries.shape[1]\n",
    "        # 线性变换并切分\n",
    "        queries = self.q_linear(queries).reshape(batch_size, seq_len, self.num_heads, self.head_length) # [batch_size, seq_len, num_heads, head_length]\n",
    "        # 转置seq_len, num_heads维度\n",
    "        queries = queries.transpose(1, 2) # [batch_size, num_heads, seq_len head_length]\n",
    "        keys = self.k_linear(keys).reshape(batch_size, seq_len, self.num_heads, self.head_length)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        values = self.v_linear(values).reshape(batch_size, seq_len, self.num_heads, self.head_length)\n",
    "        \n",
    "        # 传入attn\n",
    "        sdp_output = self.attn(queries, keys, values) # [batch_size, num_heads, seq_len, head_length]\n",
    "        # 转置回seq_len, num_heads\n",
    "        sdp_output = sdp_output.transpose(1, 2)       # [batch_size, seq_len, num_heads, head_length]\n",
    "        # concat多头\n",
    "        sdp_output = sdp_output.reshape(batch_size, seq_len, self.embed_len) \n",
    "        # output: [batch_size, seq_len, embed_len]\n",
    "        return self.output_linear(sdp_output)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca0514b8-774a-4202-86b8-8355732252a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## encoder block\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=512, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        self.multihead_attn = MultiHeadAttention()\n",
    "        self.firstnorm = nn.LayerNorm(self.embed_len)\n",
    "        self.secondnorm = nn.LayerNorm(self.embed_len)\n",
    "        self.dropout_layer = nn.Dropout(p=self.dropout)\n",
    "        \n",
    "        self.feed_forward = nn.Sequential(  nn.Linear(self.embed_len, self.embed_len*4),\n",
    "                                            nn.ReLU(),\n",
    "                                            nn.Linear(self.embed_len*4, self.embed_len))\n",
    "        \n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        queries.shape: [batch_size, seq_len, embed_dim]\n",
    "        keys.shape: [batch_size, seq_len, embed_dim]\n",
    "        values.shape: [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "        attn_output = self.multihead_attn(queries, keys, values) # [batch_size, seq_len, embed_len]\n",
    "        # Add & Norm\n",
    "        first_sublayer_output = self.firstnorm(attn_output + queries)\n",
    "        # FeedForward\n",
    "        ff_output = self.feed_forward(first_sublayer_output)\n",
    "        ff_output = self.dropout_layer(ff_output)\n",
    "        # Add & Norm\n",
    "        return self.secondnorm(ff_output + first_sublayer_output) # output: [batch_size, seq_len, embed_len]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c065f29-e10b-42bb-9158-f1d132660dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## decoder block\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, dropout=drop_out):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.maskedMultihead_attn = MultiHeadAttention(mask=True)\n",
    "        self.firstnorm = nn.LayerNorm(self.embed_len)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.encoder_block = EncoderBlock()\n",
    "        \n",
    "    def forward(self, queries, keys, values):\n",
    "        masked_multihead_attn_output = self.maskedMultihead_attn(queries, keys, values)\n",
    "        masked_multihead_attn_output = self.dropout_layer(masked_multihead_attn_output)\n",
    "        first_sublayer_output = self.firstnorm(masked_multihead_attn_output)\n",
    "        \n",
    "        return self.encoder_block(first_sublayer_output, keys, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3dc9f7e-c229-47ee-9c99-e004291e0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformer\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embed_len=embed_len, stack_len=stack_len, device=device, output_vocab_size=output_vocab_size):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embed_len = embed_len\n",
    "        self.stack_len = stack_len\n",
    "        self.device = device\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        \n",
    "        self.embedding = InputEmbedding().to(self.device)\n",
    "        self.enc_stack = nn.ModuleList(EncoderBlock() for _ in range(stack_len))\n",
    "        self.dec_stack = nn.ModuleList(DecoderBlock() for _ in range(stack_len))\n",
    "        \n",
    "        self.final_linear = nn.Linear(self.embed_len, self.output_vocab_size).to(self.device)\n",
    "        self.softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, test_input, test_output):\n",
    "        enc_output = self.embedding(test_input)\n",
    "        \n",
    "        for enc_layer in self.enc_stack:\n",
    "            enc_output = enc_layer(enc_output, enc_output, enc_output)\n",
    "        \n",
    "        dec_output = self.embedding(test_output)\n",
    "        for dec_layer in self.dec_stack:\n",
    "            dec_output = dec_layer(dec_output, enc_output, enc_output)\n",
    "        \n",
    "        final_output = self.final_linear(dec_output)\n",
    "        return self.softmax(final_output)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4dfc3f0-8521-4e31-8205-a33ebd9acd72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2569/2412380394.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self.softmax(final_output)\n"
     ]
    }
   ],
   "source": [
    "input_tokens = torch.randint(input_vocab_size, (batch_size, 30)).to(device)\n",
    "tgt_tokens = torch.randint(output_vocab_size, (batch_size, 30)).to(device)\n",
    "\n",
    "transformer = Transformer().to(device)\n",
    "transformer_output = transformer(input_tokens, tgt_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b041137-df89-425a-80a5-eb8ed7d23d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 30, 7000])\n"
     ]
    }
   ],
   "source": [
    "print(transformer_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33458432-9413-4a61-84ee-5a40c640802a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
