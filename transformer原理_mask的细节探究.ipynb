{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87390d16-080e-4619-94f2-4b36ebba61a4",
   "metadata": {},
   "source": [
    "# transformer 原理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae6c4300-380b-4ad8-9ca6-9875265e9f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "1.12.0+cu113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a06b750-d32b-40bc-b604-4a2e90d485c5",
   "metadata": {},
   "source": [
    "## encoder 部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "521de130-686e-4264-b799-e76e3dd89bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. wordEmbedding\n",
    "# 2. PositionEncoding\n",
    "# 3. encoding self-attention mask\n",
    "#             (来得到表征有效， 不希望吸收到padding的key)\n",
    "# 4. deocder 的masked Multi-head Attention\n",
    "#             (保证因果性)\n",
    "# 5. cross-attention 的mask 与encoder的mask类似，但是涉及到两个序列的mask\n",
    " \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b714e2-bbf3-4f2d-b6d0-89942bc286e4",
   "metadata": {},
   "source": [
    "### word embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d706a5d2-6ff5-4718-a096-7bbe9d6b80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 关于wordembedding, 以序列建模为例\n",
    "# 考虑source sentense 和 target sentence\n",
    "\n",
    "# 构建序列， 序列的字符以其在词表中的索引的形式表示\n",
    "batch_size = 2\n",
    "# src_len = torch.randint(2, 5, (batch_size, ))  # min, max, shape\n",
    "# tgt_len = torch.randint(2, 5, (batch_size, ))\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "\n",
    "# 模型维度，embedding维度\n",
    "model_dim = 8\n",
    "\n",
    "# 序列最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "\n",
    "src_len = torch.Tensor([2, 4]).to(torch.int32)\n",
    "tgt_len = torch.Tensor([4, 3]).to(torch.int32)\n",
    "\n",
    "# 单词索引序列表示句子，源句子和目标句子\n",
    "## pad 使一个batch的序列长度一样 在右边pad 默认pad 0.\n",
    "src_seq = torch.cat(\n",
    "    [torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, size=(L, )), (0, max(src_len)-L)), 0) \n",
    "    for L in src_len]\n",
    ")\n",
    "tgt_seq = torch.cat(\n",
    "    [torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, size=(L, )), (0, max(tgt_len)-L)), 0)\n",
    "    for L in tgt_len]\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "# 构造embedding\n",
    "## （num_embedding, embed_dim）\n",
    " \n",
    "src_embedding_table = \\\n",
    "    nn.Embedding(max_num_src_words+1, model_dim, padding_idx=0)\n",
    "tgt_embedding_table = \\\n",
    "    nn.Embedding(max_num_tgt_words+1, model_dim, padding_idx=0)\n",
    "## 每一行代表一个embedding 向量， 第0行是padding对应\n",
    "\n",
    "## 尝试调用src_embedding_table求embedding(相当于调用forward方法)\n",
    "src_embedding = src_embedding_table(src_seq)\n",
    "tgt_embedding = tgt_embedding_table(tgt_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e234db4-001e-47af-8670-d0232614a003",
   "metadata": {},
   "source": [
    "### postion embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5888f4c0-abf8-4e98-adb4-2d1dcc519c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造position_embedding\n",
    "## sin/cos 1. 泛化能力比较强， 2. 具有确定性， 3. 可以用前面的构造后面的\n",
    "## 奇数列用cos, 偶数列sin \n",
    "## gradient不用计算\n",
    "max_position_len = 5\n",
    "## pos维度是序列不同位置\n",
    "## i_mat是特征维度\n",
    "pos_mat = torch.arange(max_position_len).reshape((-1, 1))\n",
    "i_mat = torch.pow(10000, torch.arange(0, 8, 2).reshape((1, -1))/model_dim)\n",
    "\n",
    "# 计算position embedding\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "## 偶数特征维\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)\n",
    "## 奇数特征维\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)\n",
    "\n",
    "# 用nn.Embedding()封装 \n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "# 生成位置编码 就是 0， 1， 2， ... pos_idx\n",
    "src_pos = torch.cat(\n",
    "    [torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len], 0\n",
    ").to(torch.int32)\n",
    "\n",
    "tgt_pos = torch.cat(\n",
    "    [torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len], 0\n",
    ").to(torch.int32)\n",
    "\n",
    "# 生成pe向量\n",
    "src_pe = pe_embedding(src_pos)\n",
    "tgt_pe = pe_embedding(tgt_pos)\n",
    "\n",
    "# print(tgt_pe, tgt_pe.shape) # (bs, sl, fs)\n",
    "\n",
    "## 为什么用sin/cos？\n",
    "### 有泛化能力： 当inference阶段我们得到的序列长度更大，我们可以用较小的pe\n",
    "### 线性组合构造出pos更大的pe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae489d9-15ae-42bb-83e1-febcd3c986a1",
   "metadata": {},
   "source": [
    "## 构造encoder的self-attn mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16fd6bd7-392d-4c92-9bcc-a0f0a7658a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 难点！ \n",
    "# 构造encoder的self-attn mask\n",
    "# mask的shape: [batch_size, max_src_len, max_src_len], 值为1或者-inf\n",
    "# ···· 因为 我们要用 mask 与 (Q@K.T)/sqrt(d_k) 在softmax之前做elemet-wise\n",
    "# .... 的乘法，乘到-inf的元素的值softmax之后将变成0，于是注意力为0 最后@V\n",
    "\n",
    "## 要用到src_len, max_src_seq_len\n",
    "### 构建有效的编码器的位置\n",
    "valid_encoder_pos = torch.cat(\n",
    "[torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0)\n",
    " for L in src_len]\n",
    ") # 得到有效位置的序列信息\n",
    "## 扩充维度\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos, -1)\n",
    "## 做外积得到有效相关性的矩阵\n",
    "valid_encoder_mat = torch.bmm(valid_encoder_pos, valid_encoder_pos.transpose(-2, -1))\n",
    "# print(valid_encoder_mat)\n",
    "invalid_encoder_mat = (1 - valid_encoder_mat).to(torch.bool)\n",
    "## 假想一个score(Q@K.T)/sqrt(d_k)\n",
    "score = torch.randn(batch_size, max(src_len), max(src_len))\n",
    "## 在score的invalid_encoder_mat为True的地方填充-inf(1e-9)\n",
    "score = score.masked_fill(invalid_encoder_mat, -1e9)\n",
    "prob = F.softmax(score, -1)\n",
    "## pad的地方score被填充成-1e9经过softmax之后注意力变成0\n",
    "# print(src_len)\n",
    "# print(invalid_encoder_mat)\n",
    "# print(score)\n",
    "# print(prob)\n",
    "## 如果每一个位置都被mask掉，那么输出概率是一个均匀分布，\n",
    "## 但是在计算loss的时候，我们也会传入一个mask。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "502b2763-c1d9-45f0-a122-04aff16750a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax 演示\n",
    "# scaled dot-product attn 中 scale的重要性\n",
    "\n",
    "## q 和 k 的点乘， 方差变小一点 1. softmax出来的概率分布就不会特别尖锐\n",
    "##                2. 反向求导的jacobbi矩阵的数值会比较稳定，不会接近于0\n",
    "\n",
    "alpha1 = 0.1\n",
    "alpha2 = 10\n",
    "\n",
    "score = torch.randn(2, 5)\n",
    "\n",
    "# 概率分布\n",
    "prob1 = F.softmax(score*alpha1, -1)\n",
    "prob2 = F.softmax(score*alpha2, -1)\n",
    "# print (prob1)\n",
    "# print (prob2) # 发现概率分布差距变得很大（尖锐）\n",
    "\n",
    "# jacobbi 矩阵\n",
    "def softmax_func(score):\n",
    "    return F.softmax(score, -1)\n",
    "\n",
    "jaco_mat1 = torch.autograd.functional.jacobian(softmax_func, score*alpha1)\n",
    "jaco_mat2 = torch.autograd.functional.jacobian(softmax_func, score*alpha2)\n",
    "# print(jaco_mat1)\n",
    "# print(jaco_mat2) # 发现jaco_mat2的数值变得很小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879fb66-dda8-43cd-9e92-4044003348b5",
   "metadata": {},
   "source": [
    "## Decoder 部分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86771470-a94f-4b56-aee8-9a730aba08b0",
   "metadata": {},
   "source": [
    "## intro-mask的decoder 的mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efd5d193-c15e-4332-97e0-50414cddc2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.6556, -1.4813,  0.0426, -0.3118],\n",
      "         [ 0.2175,  0.5590, -1.3931, -0.3189],\n",
      "         [-1.1638, -0.0730, -1.0885, -0.6744],\n",
      "         [-0.6843,  0.6765, -1.0622, -0.4397]],\n",
      "\n",
      "        [[ 0.8388,  0.2593,  0.1865, -0.0054],\n",
      "         [-1.3871,  1.5679,  0.0312, -1.0996],\n",
      "         [-0.0787, -0.7787,  0.3698,  0.3719],\n",
      "         [-1.3029, -1.0814,  0.4262,  0.8410]]])\n",
      "tensor([[[-1.6556e+00, -1.4813e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 2.1748e-01,  5.5904e-01, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.1638e+00, -7.3021e-02, -1.0000e+09, -1.0000e+09],\n",
      "         [-6.8430e-01,  6.7645e-01, -1.0000e+09, -1.0000e+09]],\n",
      "\n",
      "        [[ 8.3881e-01,  2.5926e-01,  1.8649e-01, -5.3785e-03],\n",
      "         [-1.3871e+00,  1.5679e+00,  3.1219e-02, -1.0996e+00],\n",
      "         [-7.8655e-02, -7.7867e-01,  3.6984e-01,  3.7194e-01],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "tensor([[[0.4565, 0.5435, 0.0000, 0.0000],\n",
      "         [0.4154, 0.5846, 0.0000, 0.0000],\n",
      "         [0.2515, 0.7485, 0.0000, 0.0000],\n",
      "         [0.2041, 0.7959, 0.0000, 0.0000]],\n",
      "\n",
      "        [[0.3983, 0.2231, 0.2074, 0.1712],\n",
      "         [0.0390, 0.7482, 0.1609, 0.0519],\n",
      "         [0.2159, 0.1072, 0.3381, 0.3388],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"
     ]
    }
   ],
   "source": [
    "# 交叉attn mask\n",
    "## decoder的输出作为query，encoder的内容作为key和value\n",
    "\n",
    "\n",
    "# 构造intro-attn的mask部分, 需要考虑encoder和decoder的mask部分\n",
    "## Q@K.T shape：[batch_size, tgt_seq, src_seq]\n",
    "\n",
    "### 构建有效的编码器的位置\n",
    "valid_encoder_pos = torch.cat(\n",
    "[torch.unsqueeze(F.pad(torch.ones(L), (0, max(src_len)-L)), 0)\n",
    " for L in src_len]\n",
    ") # 得到有效位置的序列信息\n",
    "## 扩充维度\n",
    "valid_encoder_pos = torch.unsqueeze(valid_encoder_pos, -1)\n",
    "\n",
    "valid_decoder_pos = torch.cat(\n",
    "[torch.unsqueeze(F.pad(torch.ones(L), (0, max(tgt_len)-L)), 0)\n",
    " for L in tgt_len]\n",
    ")\n",
    "valid_decoder_pos = torch.unsqueeze(valid_decoder_pos, -1)\n",
    "# print(valid_encoder_pos)\n",
    "# print(valid_decoder_pos)\n",
    "\n",
    "#做bmm 做矩阵相乘\n",
    "valid_cross_pos = torch.bmm(valid_decoder_pos, valid_encoder_pos.transpose(-2, -1))\n",
    "# print(valid_cross_pos)\n",
    "\n",
    "\n",
    "invalid_cross_pos_mat = 1-valid_cross_pos\n",
    "mask_cross_attn = invalid_cross_pos_mat.to(torch.bool)\n",
    "\n",
    "## 假想有一个score shape:[batch_size, tgt_seq, src_seq]\n",
    "score = torch.randn(batch_size, max(tgt_len), max(src_len))\n",
    "masked_score = score.masked_fill(mask_cross_attn, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(score)\n",
    "print(masked_score)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1ff205-c65d-4100-846f-25bc918a4ec3",
   "metadata": {},
   "source": [
    "### decoder mask 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4c376021-603f-44af-bf68-80987c52c200",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.5939, -1.4200,  0.5034, -1.7348],\n",
      "         [-2.1207,  1.1539,  0.2158, -0.3148],\n",
      "         [-0.2288,  0.7570, -2.2965,  0.1419],\n",
      "         [ 0.1953,  0.5820,  0.2122,  0.7060]],\n",
      "\n",
      "        [[ 0.5149,  1.8388, -0.1670, -0.6262],\n",
      "         [-1.2218, -1.0102, -1.7788, -0.1309],\n",
      "         [ 1.4810, -0.9104,  0.3862, -0.3399],\n",
      "         [-1.7931,  0.6619,  0.4371, -0.3657]]])\n",
      "tensor([[[-1.5939e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-2.1207e+00,  1.1539e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [-2.2882e-01,  7.5699e-01, -2.2965e+00, -1.0000e+09],\n",
      "         [ 1.9528e-01,  5.8200e-01,  2.1218e-01,  7.0597e-01]],\n",
      "\n",
      "        [[ 5.1494e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09],\n",
      "         [-1.2218e+00, -1.0102e+00, -1.0000e+09, -1.0000e+09],\n",
      "         [ 1.4810e+00, -9.1041e-01,  3.8615e-01, -1.0000e+09],\n",
      "         [-1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09]]])\n",
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0365, 0.9635, 0.0000, 0.0000],\n",
      "         [0.2627, 0.7041, 0.0332, 0.0000],\n",
      "         [0.1940, 0.2855, 0.1973, 0.3232]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4473, 0.5527, 0.0000, 0.0000],\n",
      "         [0.7012, 0.0642, 0.2346, 0.0000],\n",
      "         [0.2500, 0.2500, 0.2500, 0.2500]]])\n"
     ]
    }
   ],
   "source": [
    "## decoder self-attn的mask\n",
    "### output 不是一次性的把所有目标序列输出出来，而是自回归的\n",
    "\n",
    "### 为了在训练阶段和inference阶段保持一致，\n",
    "### 所以要在训练阶段的每一个位置的答案mask，每次只放出要预测的时间步之前的\n",
    "### 所以这个mask应该是一个三角形的\n",
    "valid_decoder_tril_mat = torch.cat(\n",
    "[ \n",
    "    torch.unsqueeze(F.pad(torch.tril(torch.ones(L, L)), \n",
    "                     (0, max(tgt_len)-L, 0, max(tgt_len)-L)), 0)\n",
    "for L in tgt_len\n",
    "])\n",
    "# print(valid_decoder_tril_mat)\n",
    "# print(valid_decoder_tril_mat.shape)\n",
    "# 获得掩码矩阵\n",
    "invalid_decoder_pos_mat = (1-valid_decoder_tril_mat).to(torch.bool)\n",
    "# print(invalid_decoder_pos_mat)\n",
    "\n",
    "# 假想score\n",
    "score = torch.randn(batch_size, max(tgt_len), max(tgt_len))\n",
    "masked_score = score.masked_fill(invalid_decoder_pos_mat, -1e9)\n",
    "prob = F.softmax(masked_score, -1)\n",
    "print(score)\n",
    "print(masked_score)\n",
    "print(prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05eb54-6136-4ab4-a171-a7c3a144d88d",
   "metadata": {},
   "source": [
    "## attn 实现函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3771014-274d-44b0-af5e-0b187f3d2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建self_attn\n",
    "def scaled_dot_product_attn(Q, K, V, attn_mask):\n",
    "    # shape of QKV : [batch_size*num_head, seq_len, model_dim/num_head]\n",
    "    # shape of attn_mask : (sl, sl)\n",
    "    # bs = batch_size*num_head\n",
    "    score = torch.bmm(Q, K.transpose(-1, -2)) / torch.sqrt(model_dim) # (bs, sl, sl)\n",
    "    masked_score = score.masked_fill(attn_mask, -1e9) # (bs, sl, sl)\n",
    "    prob = F.softmax(masked_score, -1) # (bs, sl, sl)\n",
    "    context = torch.bmm(prob, V) # (bs, sl, model_dim/num_head)\n",
    "    return context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbe806-daf1-4779-b648-95f40d26c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## pytorch 源码中\n",
    "#### attn_mask用的加法加到attn上，attn_mask 有效的地方是0， 无效的地方是-1e9"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
